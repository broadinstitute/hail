apiVersion: v1
kind: Namespace
metadata:
  name: monitoring
---
kind: DaemonSet
apiVersion: apps/v1
metadata:
  name: node-exporter
  namespace: monitoring
spec:
  selector:
    matchLabels:
      app: node-exporter
  template:
    metadata:
      name: node-exporter
      labels:
        daemon: node-exporter
        app: node-exporter
        grafanak8sapp: "true"
    spec:
      volumes:
       - name: proc
         hostPath:
           path: /proc
       - name: sys
         hostPath:
           path: /sys
      tolerations:
       - key: preemptible
         value: "true"
      containers:
       - name: node-exporter
         image: quay.io/prometheus/node-exporter:v0.18.0
         args:
          - --path.procfs=/proc_host
          - --path.sysfs=/host_sys
         ports:
          - name: node-exporter
            hostPort: 9100
            containerPort: 9100
         volumeMounts:
          - name: sys
            readOnly: true
            mountPath: /host_sys
          - name: proc
            readOnly: true
            mountPath: /proc_host
         imagePullPolicy: IfNotPresent
      restartPolicy: Always
      hostNetwork: true
      hostPID: true
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: kube-state-metrics
  namespace: monitoring
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: kube-state-metrics
  namespace: monitoring
rules:
 - apiGroups: [""]
   resources:
   - configmaps
   - secrets
   - nodes
   - pods
   - services
   - resourcequotas
   - replicationcontrollers
   - limitranges
   - persistentvolumeclaims
   - persistentvolumes
   - namespaces
   - endpoints
   verbs: ["list", "watch"]
 - apiGroups: ["extensions"]
   resources:
   - daemonsets
   - deployments
   - replicasets
   - ingresses
   verbs: ["list", "watch"]
 - apiGroups: ["apps"]
   resources:
   - daemonsets
   - deployments
   - replicasets
   - statefulsets
   verbs: ["list", "watch"]
 - apiGroups: ["batch"]
   resources:
   - cronjobs
   - jobs
   verbs: ["list", "watch"]
 - apiGroups: ["autoscaling"]
   resources:
   - horizontalpodautoscalers
   verbs: ["list", "watch"]
 - apiGroups: ["policy"]
   resources:
   - poddisruptionbudgets
   verbs: ["list", "watch"]
 - apiGroups: ["certificates.k8s.io"]
   resources:
   - certificatesigningrequests
   verbs: ["list", "watch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: kube-state-metrics
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: kube-state-metrics
subjects:
 - kind: ServiceAccount
   name: kube-state-metrics
   namespace: monitoring
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kube-state-metrics
  namespace: monitoring
  labels:
    app: kube-state-metrics
    grafanak8sapp: "true"
spec:
  selector:
    matchLabels:
      app: kube-state-metrics
  replicas: 1
  template:
    metadata:
      labels:
        app: kube-state-metrics
        grafanak8sapp: "true" 
    spec:
      serviceAccountName: kube-state-metrics
      containers:
      - name: kube-state-metrics
        image: quay.io/coreos/kube-state-metrics:v1.6.0
        ports:
        - name: http-metrics
          containerPort: 8080
        - name: telemetry
          containerPort: 8081
        readinessProbe:
          httpGet:
            path: /healthz
            port: 8080
          initialDelaySeconds: 5
          timeoutSeconds: 5
---
apiVersion: v1
kind: Service
metadata:
  name: kube-state-metrics
  namespace: monitoring
  labels:
    app: kube-state-metrics
  annotations:
    prometheus.io/scrape: 'true'
spec:
  ports:
   - name: http-metrics
     port: 8080
     targetPort: http-metrics
     protocol: TCP
   - name: telemetry
     port: 8081
     targetPort: telemetry
     protocol: TCP
  selector:
    app: kube-state-metrics
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: prometheus
  namespace: monitoring
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  name: prometheus
  namespace: monitoring
rules:
 - apiGroups: [""]
   resources:
    - nodes
    - nodes/proxy
    - services
    - endpoints
    - pods
   verbs: ["get", "list", "watch"]
 - apiGroups:
    - extensions
   resources:
    - ingresses
   verbs: ["get", "list", "watch"]
 - nonResourceURLs: ["/metrics"]
   verbs: ["get"]
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: prometheus
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: prometheus
subjects:
 - kind: ServiceAccount
   name: prometheus
   namespace: monitoring
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: etc-prometheus
  namespace: monitoring
data:
  prometheus.yml: |
    global:
      scrape_interval: 15s
    scrape_configs:
     - job_name: 'kubernetes-kubelet'
       scheme: https
       tls_config:
         ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
         insecure_skip_verify: true
       bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
       kubernetes_sd_configs:
        - role: node
       relabel_configs:
        - action: labelmap
          regex: __meta_kubernetes_node_label_(.+)
        - target_label: __address__
          replacement: kubernetes.default.svc.cluster.local:443
        - source_labels: [__meta_kubernetes_node_name]
          regex: (.+)
          target_label: __metrics_path__
          replacement: /api/v1/nodes/${1}/proxy/metrics
     - job_name: 'kubernetes-cadvisor'
       scheme: https
       tls_config:
         ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
         insecure_skip_verify: true
       bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
       kubernetes_sd_configs:
        - role: node
       relabel_configs:
        - action: labelmap
          regex: __meta_kubernetes_node_label_(.+)
        - target_label: __address__
          replacement: kubernetes.default.svc.cluster.local:443
        - source_labels: [__meta_kubernetes_node_name]
          regex: (.+)
          target_label: __metrics_path__
          replacement: /api/v1/nodes/${1}/proxy/metrics/cadvisor
     - job_name: 'kubernetes-kube-state'
       kubernetes_sd_configs:
        - role: pod
       relabel_configs:
        - action: labelmap
          regex: __meta_kubernetes_pod_label_(.+)
        - source_labels: [__meta_kubernetes_namespace]
          action: replace
          target_label: kubernetes_namespace
        - source_labels: [__meta_kubernetes_pod_name]
          action: replace
          target_label: kubernetes_pod_name
        - source_labels: [__meta_kubernetes_pod_label_grafanak8sapp]
          regex: .*true.*
          action: keep
        - source_labels: ['__meta_kubernetes_pod_label_daemon', '__meta_kubernetes_pod_node_name']
          regex: 'node-exporter;(.*)'
          action: replace
          target_label: nodename
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    name: prometheus
  name: prometheus
  namespace: monitoring
spec:
  serviceName: "prometheus"
  selector:
    matchLabels:
      app: prometheus
  replicas: 1
  template:
    metadata:
      labels:
        app: prometheus
    spec:
      securityContext:
        fsGroup: 65534
        runAsNonRoot: true
        runAsUser: 65534
      serviceAccountName: prometheus
      containers:
       - name: prometheus
         image: prom/prometheus:v2.10.0
         imagePullPolicy: Always
         command:
          - "/bin/prometheus"
          - "--config.file=/etc/prometheus/prometheus.yml"
          - "--storage.tsdb.path=/prometheus"
          - "--web.console.libraries=/usr/share/prometheus/console_libraries"
          - "--web.console.templates=/usr/share/prometheus/consoles"
          - "--web.external-url=https://internal.hail.is/monitoring/prometheus"
         ports:
          - containerPort: 9090
            protocol: TCP
         volumeMounts:
          - mountPath: "/etc/prometheus"
            name: etc-prometheus
          - mountPath: "/prometheus"
            name: prometheus-storage
      volumes:
       - name: etc-prometheus
         configMap:
           name: etc-prometheus
  volumeClaimTemplates:
    - metadata:
        name: prometheus-storage
        namespace: monitoring
      spec:
        accessModes:
          - ReadWriteOnce
        resources:
          requests:
            storage: 50Gi
---
apiVersion: v1
kind: Service
metadata:
  name: prometheus
  namespace: monitoring
spec:
  ports:
   - port: 80
     targetPort: 9090
  selector:
    app: prometheus
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: grafana
  namespace: monitoring
  labels:
    app: grafana
spec:
  selector:
    matchLabels:
      app: grafana
  serviceName: "grafana"
  replicas: 1
  template:
    metadata:
      labels:
        app: grafana
    spec:
      securityContext:
        fsGroup: 472
        runAsNonRoot: true
        runAsUser: 472
      containers:
       - name: grafana
         image: grafana/grafana:6.2.1
         env:
          - name: GF_SERVER_DOMAIN
            value: internal.hail.is
          - name: GF_SERVER_ROOT_URL
            value: "%(protocol)s://%(domain)s/monitoring/grafana/"
         volumeMounts:
          - mountPath: /var/lib/grafana
            name: grafana-storage
         resources:
           requests:
             memory: "1G"
             cpu: "1"
         ports:
          - containerPort: 3000
  volumeClaimTemplates:
  - metadata:
      name: grafana-storage
      namespace: monitoring
    spec:
      accessModes:
        - ReadWriteOnce
      resources:
        requests:
          storage: 10Gi
---
apiVersion: v1
kind: Service
metadata:
  name: grafana
  namespace: monitoring
spec:
  ports:
   - port: 80
     targetPort: 3000
  selector:
    app: grafana
---
kind: Service
apiVersion: v1
metadata:
  name: elasticsearch
  namespace: monitoring
  labels:
    app: elasticsearch
spec:
  selector:
      app: elasticsearch
  clusterIP: None
  ports:
    - port: 9200
      name: rest
    - port: 9300
      name: inter-node
---
{% set n_replicas = 3 %}
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: es-cluster
  namespace: monitoring
spec:
  serviceName: elasticsearch
  replicas: {{ n_replicas }}
  selector:
    matchLabels:
      app: elasticsearch
  template:
    metadata:
      labels:
        app: elasticsearch
    spec:
      containers:
        - name: elasticsearch
          image: docker.elastic.co/elasticsearch/elasticsearch-oss:7.1.1
          resources:
            limits:
              cpu: 1000m
            requests:
              cpu: 100m
          ports:
            - containerPort: 9200
              name: rest
              protocol: TCP
            - containerPort: 9300
              name: inter-node
              protocol: TCP
          volumeMounts:
            - name: data
              mountPath: /usr/share/elasticsearch/data
          env:
            - name: cluster.name 
              value: gke_hail-vdc_us-central1-a_vdc
            - name: node.name
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: discovery.zen.ping.unicast.hosts
              value: "es-cluster-0.elasticsearch,es-cluster-1.elasticsearch,es-cluster-2.elasticsearch"
            - name: discovery.zen.minimum_master_nodes
              value: "{{ n_replicas // 2 + 1 }}"
            - name: cluster.initial_master_nodes
              value: "es-cluster-0,es-cluster-1,es-cluster-2"
            - name: ES_JAVA_OPTS
              value: "-Xms512m -Xmx512m"
      initContainers:
        - name: fix-permissions
          image: busybox:1.31
          command: ["sh", "-c", "chown -R 1000:1000 /usr/share/elasticsearch/data"]
          securityContext:
            privileged: true
          volumeMounts:
            - name: data
              mountPath: /usr/share/elasticsearch/data
        - name: increase-vm-max-map
          image: busybox:1.31
          command: ["sysctl", "-w", "vm.max_map_count=262144"]
          securityContext:
            privileged: true
        - name: increase-fd-ulimit
          image: busybox:1.31
          command: ["sh", "-c", "ulimit -n 65536"]
          securityContext:
            privileged: true
  volumeClaimTemplates:
  - metadata:
      name: data
      labels:
        app: elasticsearch
    spec:
      accessModes: ["ReadWriteOnce"]
      resources:
        requests:
          storage: 20Gi
---
apiVersion: v1
kind: Service
metadata:
  name: kibana
  namespace: monitoring
  labels:
    app: kibana
spec:
  ports:
    - port: 80
      targetPort: 5601
  selector:
    app: kibana
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kibana
  namespace: monitoring
  labels:
    app: kibana
spec:
  replicas: 1
  selector:
    matchLabels:
      app: kibana
  template:
    metadata:
      labels:
        app: kibana
    spec:
      volumes:
        - name: kibana-config-volume
          configMap:
            name: kibana
      containers:
        - name: kibana
          image: docker.elastic.co/kibana/kibana-oss:7.1.1
          resources:
            limits:
              cpu: 1000m
            requests:
              cpu: 100m
          volumeMounts:
            - name: kibana-config-volume
              mountPath: /usr/share/kibana/config
          ports:
            - name: http
              containerPort: 5601
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: kibana
  namespace: monitoring
  labels:
    app: kibana
data:
  kibana.yml: |-
    server:
      basePath: /monitoring/kibana
      host: 0.0.0.0
    elasticsearch:
      hosts: "http://elasticsearch:9200"
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: fluentd
  namespace: monitoring
  labels:
    app: fluentd
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: fluentd
  labels:
    app: fluentd
rules:
  - apiGroups:
      - ""
    resources:
      - pods
      - namespaces
    verbs:
      - get
      - list
      - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: fluentd
roleRef:
  kind: ClusterRole
  name: fluentd
  apiGroup: rbac.authorization.k8s.io
subjects:
  - kind: ServiceAccount
    name: fluentd
    namespace: monitoring
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluentd
  namespace: monitoring
  labels:
    app: fluentd
spec:
  selector:
    matchLabels:
      app: fluentd
  template:
    metadata:
      labels:
        app: fluentd
    spec:
      serviceAccountName: fluentd
      tolerations:
        - key: node-role.kubernetes.io/master
          effect: NoSchedule
      containers:
        - name: fluentd
          image: fluent/fluentd-kubernetes-daemonset:v1.4.2-debian-elasticsearch-1.0
          env:
            - name: FLUENT_ELASTICSEARCH_HOST
              value: "elasticsearch.monitoring.svc.cluster.local"
            - name: FLUENT_ELASTICSEARCH_PORT
              value: "9200"
            - name: FLUENT_ELASTICSEARCH_SCHEME
              value: "http"
            - name: FLUENT_UID
              value: "0"
          resources:
            limits:
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 200Mi
          volumeMounts:
            - name: varlog
              mountPath: /var/log
            - name: varlibdockercontainers
              mountPath: /var/lib/docker/containers
              readOnly: true
      terminationGracePeriodSeconds: 30
      volumes:
        - name: varlog
          hostPath:
            path: /var/log
        - name: varlibdockercontainers
          hostPath:
            path: /var/lib/docker/containers