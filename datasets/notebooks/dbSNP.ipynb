{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dbSNP\n",
    "\n",
    "Use to create Hail Tables for dbSNP, after downloading raw data from https://ftp.ncbi.nih.gov/snp/. \n",
    "\n",
    "Raw data downloaded with Hail Batch, see `hail/datasets/extract/extract_dbSNP.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hail as hl\n",
    "hl.init(spark_conf={'spark.hadoop.fs.gs.requester.pays.mode': 'AUTO', \n",
    "                    'spark.hadoop.fs.gs.requester.pays.project.id': 'broad-ctsa'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Hail Tables from GRCh37 and GRCh38 assembly reports\n",
    "\n",
    "The contigs in the VCFs are [RefSeq](https://www.ncbi.nlm.nih.gov/refseq/) accession numbers, and need to be mapped back to the appropriate chromosome for each reference genome.\n",
    "\n",
    "The GRCh37 assembly can be found [here](https://www.ncbi.nlm.nih.gov/assembly/GCF_000001405.25), and the assembly report mapping chromosomes to RefSeq sequences can be found [here](https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/001/405/GCF_000001405.25_GRCh37.p13/GCF_000001405.25_GRCh37.p13_assembly_report.txt).\n",
    "\n",
    "The GRCh38 assembly can be found [here](https://www.ncbi.nlm.nih.gov/assembly/GCF_000001405.39), and the assembly report mapping chromosomes to RefSeq sequences can be found [here](https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/001/405/GCF_000001405.39_GRCh38.p13/GCF_000001405.39_GRCh38.p13_assembly_report.txt)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GRCh37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ht = hl.import_table(\"gs://hail-datasets-tmp/dbSNP/GCF_000001405.25_GRCh37.p13_assembly_report.txt\", \n",
    "                     no_header=True, \n",
    "                     comment=\"#\",\n",
    "                     delimiter=\"\\t\", \n",
    "                     missing=\"na\")\n",
    "\n",
    "field_names = ['sequence_name','sequence_role','assigned_molecule',\n",
    "               'assigned_molecule_location/type', 'genbank_accn', 'relationship', \n",
    "               'refseq_accn', 'assembly_unit', 'sequence_length', 'ucsc_style_name']\n",
    "\n",
    "name = \"dbSNP\"\n",
    "version = \"154\"\n",
    "build = \"GRCh37\"\n",
    "n_rows = ht.count()\n",
    "n_partitions = ht.n_partitions()\n",
    "\n",
    "ht = ht.annotate_globals(\n",
    "    metadata=hl.struct(\n",
    "        name=name,\n",
    "        version=version,\n",
    "        reference_genome=build,\n",
    "        n_rows=n_rows,\n",
    "        n_partitions=n_partitions\n",
    "    )\n",
    ")\n",
    "ht = ht.rename(dict(zip([f\"f{i}\" for i in range(10)], field_names)))\n",
    "ht = ht.drop(\"relationship\").key_by(\"refseq_accn\")\n",
    "\n",
    "ht.write(\"gs://hail-datasets-us/NCBI_assembly_report_p13_GRCh37.ht\")\n",
    "ht = hl.read_table(\"gs://hail-datasets-us/NCBI_assembly_report_p13_GRCh37.ht\")\n",
    "ht.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GRCh38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ht = hl.import_table(\"gs://hail-datasets-tmp/dbSNP/GCF_000001405.39_GRCh38.p13_assembly_report.txt\", \n",
    "                     no_header=True, \n",
    "                     comment=\"#\",\n",
    "                     delimiter=\"\\t\", \n",
    "                     missing=\"na\")\n",
    "\n",
    "field_names = ['sequence_name','sequence_role','assigned_molecule',\n",
    "               'assigned_molecule_location/type', 'genbank_accn', 'relationship', \n",
    "               'refseq_accn', 'assembly_unit', 'sequence_length', 'ucsc_style_name']\n",
    "\n",
    "name = \"dbSNP\"\n",
    "version = \"154\"\n",
    "build = \"GRCh38\"\n",
    "n_rows = ht.count()\n",
    "n_partitions = ht.n_partitions()\n",
    "\n",
    "ht = ht.annotate_globals(\n",
    "    metadata=hl.struct(\n",
    "        name=name,\n",
    "        version=version,\n",
    "        reference_genome=build,\n",
    "        n_rows=n_rows,\n",
    "        n_partitions=n_partitions\n",
    "    )\n",
    ")\n",
    "ht = ht.rename(dict(zip([f\"f{i}\" for i in range(10)], field_names)))\n",
    "ht = ht.drop(\"relationship\").key_by(\"refseq_accn\")\n",
    "\n",
    "ht.write(\"gs://hail-datasets-us/NCBI_assembly_report_p13_GRCh38.ht\")\n",
    "ht = hl.read_table(\"gs://hail-datasets-us/NCBI_assembly_report_p13_GRCh38.ht\")\n",
    "ht.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Hail Tables for dbSNP\n",
    "\n",
    "Now we can use the assembly report for each reference genome build to map from RefSeq accession numbers to chromosomes, and create Hail Tables. There are no samples or entries in the dbSNP VCFs. \n",
    "\n",
    "We will create two Hail Tables for each reference genome build, both keyed by `[\"locus\", \"alleles\"]`:\n",
    "\n",
    "  - Table with all fields from the imported VCF (e.g. `gs://hail-datasets-us/dbSNP_154_GRCh37.ht`)\n",
    "  - Table with only the rsID field (e.g. `gs://hail-datasets-us/dbSNP_rsids_154_GRCh37.ht`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load NCBI assembly reports with RefSeq mappings\n",
    "assembly37_ht = hl.read_table(\"gs://hail-datasets-us/NCBI_assembly_report_p13_GRCh37.ht\")\n",
    "assembly38_ht = hl.read_table(\"gs://hail-datasets-us/NCBI_assembly_report_p13_GRCh38.ht\")\n",
    "\n",
    "# Map RefSeq identifiers to chromosomes for GRCh37 using the \"assigned_molecule\" field in assembly report\n",
    "rg37 = hl.get_reference(\"GRCh37\")\n",
    "refseq_to_chr37 = dict(zip(assembly37_ht.refseq_accn.collect(), assembly37_ht.assigned_molecule.collect()))\n",
    "refseq_to_chr37 = {k: v for k, v in refseq_to_chr37.items() if v in rg37.contigs}\n",
    "\n",
    "# Map RefSeq identifiers to chromosomes for GRCh38 using the \"ucsc_style_name\" field in assembly report\n",
    "rg38 = hl.get_reference(\"GRCh38\")\n",
    "refseq_to_chr38 = dict(zip(assembly38_ht.refseq_accn.collect(), assembly38_ht.ucsc_style_name.collect()))\n",
    "refseq_to_chr38 = {k: v.split(\"_\")[0] for k, v in refseq_to_chr38.items() if v in rg38.contigs and \"chrUn\" not in v}\n",
    "refseq_to_chr38.pop(None, None)\n",
    "\n",
    "recodings = {\"GRCh37\": refseq_to_chr37, \n",
    "             \"GRCh38\": refseq_to_chr38}\n",
    "\n",
    "# For use in filenames/metadata\n",
    "name = \"dbSNP\"\n",
    "version = \"154\"\n",
    "\n",
    "for build in [\"GRCh37\", \"GRCh38\"]:\n",
    "    mt = hl.import_vcf(f\"gs://hail-datasets-tmp/{name}/{name}_{version}_{build}.vcf.bgz\", \n",
    "                   contig_recoding=recodings[build],\n",
    "                   skip_invalid_loci=True,\n",
    "                   reference_genome=build)\n",
    "    \n",
    "    # No samples or entries, just grab table with the rows and write the full table\n",
    "    ht = mt.rows()\n",
    "    ht = ht.repartition(512, shuffle=True)\n",
    "    ht = ht.checkpoint(f\"gs://hail-datasets-tmp/checkpoints/{name}_{version}_{build}_repartitioned.ht\", \n",
    "                       _read_if_exists=True,\n",
    "                       overwrite=False)\n",
    "\n",
    "    n_rows = ht.count()\n",
    "    n_partitions = ht.n_partitions()\n",
    "\n",
    "    ht = ht.annotate_globals(\n",
    "        metadata=hl.struct(\n",
    "            name=name,\n",
    "            version=version,\n",
    "            reference_genome=build,\n",
    "            n_rows=n_rows,\n",
    "            n_partitions=n_partitions\n",
    "        )\n",
    "    )\n",
    "    ht.write(f\"gs://hail-datasets-us/{name}_{version}_{build}.ht\")\n",
    "    ht = hl.read_table(f\"gs://hail-datasets-us/{name}_{version}_{build}.ht\")\n",
    "    ht.describe()\n",
    "    \n",
    "    # Write table with only rsid's\n",
    "    ht_rsid = hl.read_table(f\"gs://hail-datasets-us/{name}_{version}_{build}.ht\")\n",
    "    ht_rsid = ht_rsid.select(\"rsid\")\n",
    "\n",
    "    n_rows = ht_rsid.count()\n",
    "    n_partitions = ht_rsid.n_partitions()\n",
    "\n",
    "    ht_rsid = ht_rsid.annotate_globals(\n",
    "        metadata=hl.struct(\n",
    "            name=f\"{name}_rsids\",\n",
    "            version=version,\n",
    "            reference_genome=build,\n",
    "            n_rows=n_rows,\n",
    "            n_partitions=n_partitions\n",
    "        )\n",
    "    )\n",
    "    ht_rsid.write(f\"gs://hail-datasets-us/{name}_rsids_{version}_{build}.ht\")\n",
    "    ht_rsid = hl.read_table(f\"gs://hail-datasets-us/{name}_rsids_{version}_{build}.ht\")\n",
    "    ht_rsid.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add dbSNP to datasets API and annotation DB\n",
    "\n",
    "Now we can add the tables we created to `hail/python/hail/experimental/datasets.json` and create schemas for the docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "output_dir = os.path.abspath(\"../../hail/python/hail/docs/datasets/schemas\")\n",
    "datasets_path = os.path.abspath(\"../../hail/python/hail/experimental/datasets.json\")\n",
    "with open(datasets_path, \"r\") as f:\n",
    "    datasets = json.load(f)\n",
    "\n",
    "names = [\"dbSNP\", \"dbSNP_rsids\"]\n",
    "version = \"154\"\n",
    "builds = [\"GRCh37\", \"GRCh38\"]\n",
    "\n",
    "gcs_us_url_root = \"gs://hail-datasets-us\"\n",
    "gcs_eu_url_root = \"gs://hail-datasets-eu\"\n",
    "aws_us_url_root = \"s3://hail-datasets-us-east-1\"\n",
    "\n",
    "for name in names:\n",
    "    json_entry = {\n",
    "        \"annotation_db\": {\n",
    "            \"key_properties\": []\n",
    "        },\n",
    "        \"description\": \"dbSNP: Reference SNP (rs or RefSNP) Hail Table. The database includes both common and rare single-base nucleotide variation (SNV), short (=< 50bp) deletion/insertion polymorphisms, and other classes of small genetic variations.\",\n",
    "        \"url\": \"https://www.ncbi.nlm.nih.gov/snp/docs/RefSNP_about/\",\n",
    "        \"versions\": [\n",
    "            {\n",
    "                \"reference_genome\": builds[0],\n",
    "                \"url\": {\n",
    "                    \"aws\": {\n",
    "                        \"us\": f\"{aws_us_url_root}/{name}_{version}_{builds[0]}.ht\"\n",
    "                    },\n",
    "                    \"gcp\": {\n",
    "                        \"eu\": f\"{gcs_eu_url_root}/{name}_{version}_{builds[0]}.ht\",\n",
    "                        \"us\": f\"{gcs_us_url_root}/{name}_{version}_{builds[0]}.ht\"\n",
    "                    }\n",
    "                },\n",
    "                \"version\": version\n",
    "            },\n",
    "            {\n",
    "                \"reference_genome\": builds[1],\n",
    "                \"url\": {\n",
    "                    \"aws\": {\n",
    "                        \"us\": f\"{aws_us_url_root}/{name}_{version}_{builds[1]}.ht\"\n",
    "                    },\n",
    "                    \"gcp\": {\n",
    "                        \"eu\": f\"{gcs_eu_url_root}/{name}_{version}_{builds[1]}.ht\",\n",
    "                        \"us\": f\"{gcs_us_url_root}/{name}_{version}_{builds[1]}.ht\"\n",
    "                    }\n",
    "                },\n",
    "                \"version\": version\n",
    "            }            \n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    if name == \"dbSNP_rsids\":\n",
    "        json_entry[\"description\"] = \"dbSNP: This Hail Table only contains a mapping from Reference SNP IDs (rsID) to locus and alleles. For full dataset, see dbSNP.\"\n",
    "    \n",
    "    datasets[name] = json_entry\n",
    "\n",
    "# Write new entries back to datasets.json config:\n",
    "with open(datasets_path, \"w\") as f:\n",
    "    json.dump(datasets, f, sort_keys=True, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {k: v for k, v in datasets.items() if \"dbSNP\" in k}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify we can load tables\n",
    "datasets_path = os.path.abspath(\"../../hail/python/hail/experimental/datasets.json\")\n",
    "with open(datasets_path, \"r\") as f:\n",
    "    datasets = json.load(f)\n",
    "\n",
    "# regions = [(\"aws\", \"us\"), (\"gcp\", \"us\"), (\"gcp\", \"eu\")]\n",
    "regions = [(\"gcp\", \"us\"), (\"gcp\", \"eu\")]\n",
    "for version in datasets[\"dbSNP\"][\"versions\"]:\n",
    "    v = version[\"version\"]\n",
    "    if v == \"154\":\n",
    "        rg = version[\"reference_genome\"]\n",
    "        print(f\"v{v}_{rg}\")\n",
    "        for r in regions:\n",
    "            cloud, region = r\n",
    "            print(f\"{cloud}_{region}\")\n",
    "            url = version[\"url\"][cloud][region]\n",
    "            print(url)\n",
    "            ht = hl.read_table(url)\n",
    "            ht.describe()\n",
    "            print(\"\\n\")\n",
    "            \n",
    "for version in datasets[\"dbSNP_rsids\"][\"versions\"]:\n",
    "    v = version[\"version\"]\n",
    "    if v == \"154\":\n",
    "        rg = version[\"reference_genome\"]\n",
    "        print(f\"v{v}_{rg}\")\n",
    "        for r in regions:\n",
    "            cloud, region = r\n",
    "            print(f\"{cloud}_{region}\")\n",
    "            url = version[\"url\"][cloud][region]\n",
    "            print(url)\n",
    "            ht = hl.read_table(url)\n",
    "            ht.describe()\n",
    "            print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create/update schema .rst file\n",
    "import textwrap\n",
    "\n",
    "output_dir = os.path.abspath(\"../../hail/python/hail/docs/datasets/schemas\")\n",
    "datasets_path = os.path.abspath(\"../../hail/python/hail/experimental/datasets.json\")\n",
    "with open(datasets_path, \"r\") as f:\n",
    "    datasets = json.load(f)\n",
    "\n",
    "names = [\"dbSNP\", \"dbSNP_rsids\"]\n",
    "for name in names:\n",
    "    versions = sorted(set(dataset[\"version\"] for dataset in datasets[name][\"versions\"]))\n",
    "    if not versions:\n",
    "        versions = [None]\n",
    "    reference_genomes = sorted(set(dataset[\"reference_genome\"] for dataset in datasets[name][\"versions\"]))\n",
    "    if not reference_genomes:\n",
    "        reference_genomes = [None]\n",
    "\n",
    "    print(name)\n",
    "    print(versions[0])\n",
    "    print(reference_genomes[0] + \"\\n\")\n",
    "\n",
    "    path = [dataset[\"url\"][\"gcp\"][\"us\"]\n",
    "            for dataset in datasets[name][\"versions\"]\n",
    "            if all([dataset[\"version\"] == versions[0],\n",
    "                    dataset[\"reference_genome\"] == reference_genomes[0]])]\n",
    "    assert len(path) == 1\n",
    "    path = path[0]\n",
    "    if path.endswith(\".ht\"):\n",
    "        table = hl.methods.read_table(path)\n",
    "        table_class = \"hail.Table\"\n",
    "    else:\n",
    "        table = hl.methods.read_matrix_table(path)\n",
    "        table_class = \"hail.MatrixTable\"\n",
    "\n",
    "    description = table.describe(handler=lambda x: str(x)).split(\"\\n\")\n",
    "    description = \"\\n\".join([line.rstrip() for line in description])\n",
    "\n",
    "    template = \"\"\".. _{dataset}:\n",
    "\n",
    "{dataset}\n",
    "{underline1}\n",
    "\n",
    "*  **Versions:** {versions}\n",
    "*  **Reference genome builds:** {ref_genomes}\n",
    "*  **Type:** :class:`{class}`\n",
    "\n",
    "Schema ({version0}, {ref_genome0})\n",
    "{underline2}\n",
    "\n",
    ".. code-block:: text\n",
    "\n",
    "{schema}\n",
    "\n",
    "\"\"\"\n",
    "    context = {\n",
    "        \"dataset\": name,\n",
    "        \"underline1\": len(name) * \"=\",\n",
    "        \"version0\": versions[0],\n",
    "        \"ref_genome0\": reference_genomes[0],\n",
    "        \"versions\": \", \".join([str(version) for version in versions]),\n",
    "        \"ref_genomes\": \", \".join([str(reference_genome) for reference_genome in reference_genomes]),\n",
    "        \"underline2\": len(\"\".join([\"Schema (\", str(versions[0]), \", \", str(reference_genomes[0]), \")\"])) * \"~\",\n",
    "        \"schema\": textwrap.indent(description, \"    \"),\n",
    "        \"class\": table_class\n",
    "    }\n",
    "    with open(output_dir + f\"/{name}.rst\", \"w\") as f:\n",
    "        f.write(template.format(**context).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
